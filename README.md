# rowan-nlp
Materials for Natural Language Processing, Rowan University, Spring 2023.


---

### _Readings_

\begin{itemize}
\item \textbf{R1}: Efficient Estimation of Word Representations in Vector Space. Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean. \url{https://arxiv.org/abs/1301.3781}.
\item \textbf{R2}: GloVe: Global Vectors for Word Representation. \url{https://nlp.stanford.edu/pubs/glove.pdf}.
\item \textbf{R3}: Bag of Tricks for Efficient Text Classification. Armand Joulin, Edouard Grave, Piotr Bojanowski, Tomas Mikolov. \url{https://arxiv.org/abs/1607.01759}.
\item \textbf{R4}: All-but-the-top: Simple and Effective Post-processing for Word Representations. Jiaqi Mu, Suma Bhat, Pramod Viswanath. \url{https://arxiv.org/abs/1702.01417}.
\item \textbf{R5}: Text Understanding from Scratch. Xiang Zhang, Yann LeCun. \url{https://arxiv.org/abs/1502.01710}.
\item \textbf{R6}: A Simple But Tough-to-Beat Baseline for Sentence Embeddings. Sanjeev Arora, Yingyu Liang, Tengyu Ma. \url{https://openreview.net/pdf?id=SyK00v5xx}.
\item \textbf{R7}: Parameter-free Sentence Embedding via Orthogonal Basis. Ziyi Yang, Chenguang Zhu, Weizhu Chen. \url{https://arxiv.org/abs/1810.00438}.
\item \textbf{R8}: Long Short-term Memory. Sepp Hochreiter, Jurgen Schmidhuber. \url{https://blog.xpgreat.com/file/lstm.pdf}.
\item \textbf{R9}: Supervised Learning of Universal Sentence Representations from Natural Language Inference Data. Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, Antoine Bordes. \url{https://arxiv.org/abs/1705.02364}.

\item \textbf{R10}: Attention is All You Need! Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. \url{https://arxiv.org/abs/1706.03762}.
\item \textbf{R11}: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova. \url{https://arxiv.org/abs/1810.04805}.

\item \textbf{R12}: DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf. \url{https://arxiv.org/abs/1910.01108}.

\item \textbf{R13}: RoBERTa: A Robustly Optimized BERT Pretraining Approach. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov. \url{https://arxiv.org/abs/1907.11692}.

\item \textbf{R14}: Improving Language Understanding
by Generative Pre-Training. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever. \url{https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf}.

\item \textbf{R15}: Language Models are Unsupervised Multitask Learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever. \url{https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf}.

\item \textbf{R16}: BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer. \url{https://arxiv.org/abs/1910.13461}.

\item \textbf{R17}: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu. \url{https://arxiv.org/pdf/1910.10683.pdf}.


\item \textbf{R18}: Language Models as Knowledge Bases? Fabio Petroni, Tim Rocktaschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H. Miller, Sebastian Riedel.
\url{https://aclanthology.org/D19-1250.pdf}.

\item \textbf{R19}: Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, But Cannot Fly. Nora Kassner, Hinrish Schutze. \url{https://arxiv.org/abs/1911.03343}.

\item \textbf{R20}: A Review on Language Models as Knowledge Bases. Badr AlKhamissi, Millicent Li, Asli Celikyilmaz, Mona Diab, Marjan Ghazvininejad. \url{https://arxiv.org/abs/2204.06031}.

\item \textbf{R21}: Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig. \url{https://arxiv.org/abs/2107.13586} and \url{http://pretrain.nlpedia.ai/}.


\item \textbf{R22}: Learning bilingual word embeddings with (almost) no bilingual data. Mikel Artetxe, Gorka Labaka, Eneko Agirre. \url{https://aclanthology.org/P17-1042/}.

\item \textbf{R23}: Unsupervised Machine Translation Using Monolingual Corpora Only. Guillaume Lample, Alexis Conneau, Ludovic Denoyer, Marc'Aurelio Ranzato. \url{https://arxiv.org/abs/1711.00043}. 

\item \textbf{R24}: On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, Shmargaret Shmitchell. \url{https://dl.acm.org/doi/10.1145/3442188.3445922}

\end{itemize}
