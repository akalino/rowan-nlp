# Natural Language Processing, Rowan University
## Materials for Natural Language Processing, Rowan University, Spring 2023.


### _Readings_

Efficient Estimation of Word Representations in Vector Space. Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean. https://arxiv.org/abs/1301.3781
Assigned Presenter:

GloVe: Global Vectors for Word Representation. https://nlp.stanford.edu/pubs/glove.pdf
Assigned Presenter:

Bag of Tricks for Efficient Text Classification. Armand Joulin, Edouard Grave, Piotr Bojanowski, Tomas Mikolov. https://arxiv.org/abs/1607.01759
Assigned Presenter:

All-but-the-top: Simple and Effective Post-processing for Word Representations. Jiaqi Mu, Suma Bhat, Pramod Viswanath. https://arxiv.org/abs/1702.01417
Assigned Presenter:

Text Understanding from Scratch. Xiang Zhang, Yann LeCun. https://arxiv.org/abs/1502.01710
Assigned Presenter:

A Simple But Tough-to-Beat Baseline for Sentence Embeddings. Sanjeev Arora, Yingyu Liang, Tengyu Ma. https://openreview.net/pdf?id=SyK00v5xx
Assigned Presenter:

Parameter-free Sentence Embedding via Orthogonal Basis. Ziyi Yang, Chenguang Zhu, Weizhu Chen. https://arxiv.org/abs/1810.00438
Assigned Presenter:

Long Short-term Memory. Sepp Hochreiter, Jurgen Schmidhuber. https://blog.xpgreat.com/file/lstm.pdf
Assigned Presenter:

Supervised Learning of Universal Sentence Representations from Natural Language Inference Data. Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, Antoine Bordes. https://arxiv.org/abs/1705.02364
Assigned Presenter:

Attention is All You Need! Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. https://arxiv.org/abs/1706.03762
Assigned Presenter:

BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova. https://arxiv.org/abs/1810.04805
Assigned Presenter:

DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf. https://arxiv.org/abs/1910.01108
Assigned Presenter:

RoBERTa: A Robustly Optimized BERT Pretraining Approach. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov. https://arxiv.org/abs/1907.11692
Assigned Presenter:

Improving Language Understanding
by Generative Pre-Training. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever. https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf
Assigned Presenter:

Language Models are Unsupervised Multitask Learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever. https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf
Assigned Presenter:

BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer. https://arxiv.org/abs/1910.13461
Assigned Presenter:

Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu. https://arxiv.org/pdf/1910.10683.pdf
Assigned Presenter:


Language Models as Knowledge Bases? Fabio Petroni, Tim Rocktaschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H. Miller, Sebastian Riedel.
https://aclanthology.org/D19-1250.pdf
Assigned Presenter:

Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, But Cannot Fly. Nora Kassner, Hinrish Schutze. https://arxiv.org/abs/1911.03343
Assigned Presenter:

A Review on Language Models as Knowledge Bases. Badr AlKhamissi, Millicent Li, Asli Celikyilmaz, Mona Diab, Marjan Ghazvininejad. https://arxiv.org/abs/2204.06031
Assigned Presenter:

Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig. https://arxiv.org/abs/2107.13586 and http://pretrain.nlpedia.ai/
Assigned Presenter:

Learning bilingual word embeddings with (almost) no bilingual data. Mikel Artetxe, Gorka Labaka, Eneko Agirre. https://aclanthology.org/P17-1042/
Assigned Presenter:

Unsupervised Machine Translation Using Monolingual Corpora Only. Guillaume Lample, Alexis Conneau, Ludovic Denoyer, Marc'Aurelio Ranzato. https://arxiv.org/abs/1711.00043}
Assigned Presenter:

On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, Shmargaret Shmitchell. https://dl.acm.org/doi/10.1145/3442188.3445922
Assigned Presenter:
